"""
Consolidated JSON Export Manager
Replaces multiple file exports with single comprehensive file using reference-based architecture
Now with configurable export modes and smart view generation
Enhanced with adaptive data quality classification for intelligent processing
"""
import json
import os
from datetime import datetime
from typing import Dict, Any, List, Optional
import logging
from pathlib import Path

from .enhanced_data_models import Destination, Theme, Evidence, TemporalSlice
from .safe_dict_utils import safe_get, safe_get_confidence_value, safe_get_nested, safe_get_dict
from .confidence_scoring import ConfidenceBreakdown, ConfidenceLevel
from .export_config import ExportConfig, ExportMode, SmartViewGenerator, create_export_config_from_yaml
from .authentic_insights_deduplicator import deduplicate_authentic_insights_for_export, AuthenticInsightsDeduplicator
from .adaptive_data_quality_classifier import AdaptiveDataQualityClassifier

logger = logging.getLogger(__name__)

class ConsolidatedJsonExportManager:
    """
    Consolidated export manager that creates a single comprehensive JSON file
    with reference-based architecture to eliminate duplication
    Now supports configurable export modes and smart view generation
    Includes authentic insights deduplication to fix cross-referencing issues
    Enhanced with adaptive data quality classification for intelligent processing
    """
    
    def __init__(self, export_base_path: str, config: Optional[Dict[str, Any]] = None):
        """
        Initialize export manager with adaptive configuration
        
        Args:
            export_base_path: Base path for exports
            config: Application configuration dictionary for adaptive processing
        """
        self.base_path = Path(export_base_path)
        self.consolidated_path = self.base_path / "consolidated"
        self.consolidated_path.mkdir(parents=True, exist_ok=True)
        
        self.logger = logging.getLogger(__name__)
        
        # Store configuration for adaptive processing
        self.config = config or {}
        
        # Initialize adaptive data quality classifier
        self.quality_classifier = AdaptiveDataQualityClassifier(self.config)
        
        # Create export configuration (can be overridden by adaptive logic)
        self.export_config = create_export_config_from_yaml(self.config)
        
        # Initialize smart view generator and insights deduplicator
        self.view_generator = SmartViewGenerator(self.export_config)
        self.insights_deduplicator = AuthenticInsightsDeduplicator()
        
        self.logger.info(f"Consolidated JSON export manager initialized with adaptive intelligence")
        self.logger.info(f"Export path: {self.consolidated_path}")
        
        # Log adaptive configuration status
        heuristics_enabled = safe_get_nested(self.config, ["data_quality_heuristics", "enabled"], False)
        self.logger.info(f"Adaptive data quality classification: {'enabled' if heuristics_enabled else 'disabled'}")

    def export_destination_insights(
        self, 
        destination: Destination,
        analysis_metadata: Optional[Dict[str, Any]] = None,
        evidence_registry: Optional[Dict[str, Dict[str, Any]]] = None
    ) -> str:
        """
        Export destination insights with adaptive processing based on data quality
        
        Args:
            destination: Destination object to export
            analysis_metadata: Optional metadata about the analysis process
            evidence_registry: Optional evidence registry for reference-based export
            
        Returns:
            str: Path to the exported file
        """
        
        try:
            # Generate safe destination ID for filename
            # Handle both objects and dictionaries for destination ID
            if hasattr(destination, 'id'):
                destination_id = destination.id
            elif isinstance(destination, dict):
                destination_id = destination.get('id', 'unknown_destination')
            else:
                destination_id = 'unknown_destination'
            
            destination_id_safe = destination_id.replace(' ', '_').replace(',', '_').lower()
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            
            # Build evidence registry if not provided
            if not evidence_registry:
                evidence_registry = self._build_evidence_registry(destination)
            
            # Classify data quality for adaptive processing
            discovered_themes_count = len(destination.themes)
            content_list = self._extract_content_from_evidence(evidence_registry)
            
            data_quality_result = self.quality_classifier.classify_data_quality(
                destination_name=destination_id,
                evidence_list=list(evidence_registry.values()),
                content_list=content_list,
                discovered_themes_count=discovered_themes_count,
                analysis_metadata=analysis_metadata
            )
            
            # Extract adaptive settings
            classification = data_quality_result["classification"]
            adaptive_settings = data_quality_result["adaptive_settings"]
            
            self.logger.info(f"Data quality classification: {classification}")
            self.logger.info(f"Reasoning: {data_quality_result['reasoning']}")
            self.logger.debug(f"Adaptive settings: {adaptive_settings}")
            
            # Update export configuration with adaptive settings
            self._apply_adaptive_settings(adaptive_settings)
            
            # Apply adaptive theme filtering
            destination = self._apply_adaptive_theme_filtering(destination, adaptive_settings)
            
            # Create export based on adaptive mode
            export_mode = adaptive_settings.get("export_mode", "themes_focused")
            
            if export_mode == "comprehensive":
                export_data = self._create_comprehensive_export(
                    destination, analysis_metadata, evidence_registry
                )
            elif export_mode == "minimal":
                export_data = self._create_minimal_export(
                    destination, evidence_registry
                )
            elif export_mode == "summary_only":
                export_data = self._create_summary_export(
                    destination, evidence_registry
                )
            elif export_mode == "evidence_focused":
                export_data = self._create_evidence_focused_export(
                    destination, evidence_registry
                )
            elif export_mode == "themes_focused":
                export_data = self._create_themes_focused_export(
                    destination, evidence_registry
                )
            else:
                # Fallback to themes_focused
                export_data = self._create_themes_focused_export(
                    destination, evidence_registry
                )
            
            # Add data quality analysis to export metadata
            if "metadata" not in export_data:
                export_data["metadata"] = {}
            export_data["metadata"]["data_quality_analysis"] = data_quality_result
            
            # Apply adaptive filtering
            export_data = self._apply_adaptive_filters(export_data, adaptive_settings)
            
            # Save export file with classification suffix
            mode_suffix = f"{export_mode}_{classification}"
            comprehensive_path = self.consolidated_path / f"dest_{destination_id_safe}_{mode_suffix}_{timestamp}.json"
            self._save_json(export_data, comprehensive_path)
            
            # Create latest symlink for easy access
            latest_link = self.consolidated_path / f"dest_{destination_id_safe}_latest.json"
            self._create_latest_link(comprehensive_path, latest_link)
            
            self.logger.info(f"Successfully exported {export_mode} insights ({classification}) to {comprehensive_path}")
            
            # Archive old exports if needed
            self._archive_old_exports(destination_id_safe, days_to_keep=7)
            
        except Exception as e:
            self.logger.error(f"Error exporting destination insights: {e}")
            raise
            
        return str(comprehensive_path)
    
    def _apply_adaptive_settings(self, adaptive_settings: Dict[str, Any]):
        """Apply adaptive settings to export configuration"""
        
        # Update confidence threshold
        if "confidence_threshold" in adaptive_settings:
            self.export_config.min_confidence_threshold = adaptive_settings["confidence_threshold"]
        
        # Update evidence per theme limit
        if "max_evidence_per_theme" in adaptive_settings:
            self.export_config.max_evidence_per_theme = adaptive_settings["max_evidence_per_theme"]
        
        # Update export mode
        if "export_mode" in adaptive_settings:
            try:
                self.export_config.mode = ExportMode(adaptive_settings["export_mode"])
            except ValueError:
                self.logger.warning(f"Invalid export mode: {adaptive_settings['export_mode']}, using default")
    
    def _apply_adaptive_theme_filtering(
        self, 
        destination: Destination, 
        adaptive_settings: Dict[str, Any]
    ) -> Destination:
        """Apply adaptive theme filtering based on data quality"""
        
        max_themes = adaptive_settings.get("max_themes", 35)
        confidence_threshold = adaptive_settings.get("confidence_threshold", 0.55)
        
        # Filter and limit themes
        filtered_themes = []
        for theme in destination.themes:
            # Calculate confidence
            if theme.confidence_breakdown:
                # Handle ConfidenceBreakdown objects, dictionaries, and JSON strings
                if hasattr(theme.confidence_breakdown, 'overall_confidence'):
                    theme_confidence = theme.confidence_breakdown.overall_confidence
                elif isinstance(theme.confidence_breakdown, dict):
                    theme_confidence = safe_get_confidence_value(theme.confidence_breakdown, "overall_confidence", 0.5)
                elif isinstance(theme.confidence_breakdown, str):
                    # Handle JSON string case
                    try:
                        import json
                        conf_dict = json.loads(theme.confidence_breakdown)
                        theme_confidence = safe_get(conf_dict, 'overall_confidence', 0.5)
                    except (json.JSONDecodeError, AttributeError):
                        theme_confidence = 0.5
                else:
                    theme_confidence = 0.5
            else:
                theme_confidence = 0.5
            
            if theme_confidence >= confidence_threshold:
                filtered_themes.append(theme)
        
        # Sort themes by confidence
        def get_theme_confidence(t):
            if not t.confidence_breakdown:
                return 0.5
            if hasattr(t.confidence_breakdown, 'overall_confidence'):
                return t.confidence_breakdown.overall_confidence
            elif isinstance(t.confidence_breakdown, dict):
                return safe_get_confidence_value(t.confidence_breakdown, "overall_confidence", 0.5)
            elif isinstance(t.confidence_breakdown, str):
                # Handle JSON string case
                try:
                    import json
                    conf_dict = json.loads(t.confidence_breakdown)
                    return safe_get(conf_dict, 'overall_confidence', 0.5)
                except (json.JSONDecodeError, AttributeError):
                    return 0.5
            else:
                return 0.5
        
        filtered_themes.sort(key=get_theme_confidence, reverse=True)
        
        if len(filtered_themes) > max_themes:
            filtered_themes = filtered_themes[:max_themes]
            self.logger.info(f"Limited themes to top {max_themes} based on adaptive settings")
        
        # Create new destination with filtered themes
        destination.themes = filtered_themes
        return destination
    
    def _apply_adaptive_filters(
        self, 
        export_data: Dict[str, Any], 
        adaptive_settings: Dict[str, Any]
    ) -> Dict[str, Any]:
        """Apply adaptive filtering to export data"""
        
        # Apply confidence threshold filtering
        confidence_threshold = adaptive_settings.get("confidence_threshold", 0.55)
        if "data" in export_data and "themes" in export_data["data"]:
            filtered_themes = {}
            for theme_id, theme_data in export_data["data"]["themes"].items():
                confidence = self._get_theme_confidence_safe(theme_data)
                if confidence >= confidence_threshold:
                    filtered_themes[theme_id] = theme_data
            export_data["data"]["themes"] = filtered_themes
        
        # Apply evidence per theme limit
        max_evidence_per_theme = adaptive_settings.get("max_evidence_per_theme", 5)
        if (max_evidence_per_theme and 
            "relationships" in export_data and 
            "theme_evidence" in export_data["relationships"]):
            
            for theme_id, evidence_ids in export_data["relationships"]["theme_evidence"].items():
                if len(evidence_ids) > max_evidence_per_theme:
                    # Keep highest authority evidence
                    evidence_registry = safe_get_nested(export_data, ["data", "evidence"], {})
                    sorted_evidence = sorted(
                        evidence_ids,
                        key=lambda eid: safe_get_nested(evidence_registry, [eid, "authority_weight"], 0),
                        reverse=True
                    )
                    export_data["relationships"]["theme_evidence"][theme_id] = sorted_evidence[:max_evidence_per_theme]
        
        return export_data
    
    def _build_evidence_registry(self, destination: Destination) -> Dict[str, Dict[str, Any]]:
        """Build evidence registry from destination themes"""
        
        evidence_registry = {}
        for theme in destination.themes:
            for evidence in theme.evidence:
                if evidence.id not in evidence_registry:
                    # Proper type checking for evidence
                    if isinstance(evidence, dict):
                        evidence_registry[evidence.id] = evidence
                    elif hasattr(evidence, 'to_dict') and callable(getattr(evidence, 'to_dict')):
                        evidence_registry[evidence.id] = evidence.to_dict()
                    else:
                        evidence_registry[evidence.id] = {"error": "unexpected_evidence_type", "type": str(type(evidence))}
        
        return evidence_registry
    
    def _extract_content_from_evidence(self, evidence_registry: Dict[str, Dict[str, Any]]) -> List[Dict[str, Any]]:
        """Extract content list from evidence registry for quality classification"""
        
        content_list = []
        for evidence_data in evidence_registry.values():
            content_list.append({
                "content": evidence_data.get("text_snippet", ""),
                "source_url": evidence_data.get("source_url", ""),
                "authority_weight": evidence_data.get("authority_weight", 0.0)
            })
        
        return content_list
    
    def _create_comprehensive_export(
        self, 
        destination: Destination,
        analysis_metadata: Optional[Dict[str, Any]] = None,
        evidence_registry: Optional[Dict[str, Dict[str, Any]]] = None
    ) -> Dict[str, Any]:
        """Create comprehensive export with all data using references"""
        
        # Deduplicate authentic insights and get cross-references
        self.insights_deduplicator.clear_registry()  # Clear previous state
        deduplicated_insights, insights_cross_refs = deduplicate_authentic_insights_for_export(destination)
        
        # Format themes and get data
        themes_data = self._format_themes_with_references(destination.themes)
        destination_data = self._format_destination_data(destination)

        # Build evidence_registry if not provided or empty
        current_evidence_registry = evidence_registry or {}
        if not current_evidence_registry:
            self.logger.info("Evidence registry not provided or empty, building from destination themes.")
            temp_registry = {}
            for theme in destination.themes:
                for ev in theme.evidence:
                    if ev.id not in temp_registry:
                        # Proper type checking for evidence
                        if isinstance(ev, dict):
                            temp_registry[ev.id] = ev
                        elif hasattr(ev, 'to_dict') and callable(getattr(ev, 'to_dict')):
                            temp_registry[ev.id] = ev.to_dict()
                        else:
                            temp_registry[ev.id] = {"error": "unexpected_evidence_type", "type": str(type(ev))}
            current_evidence_registry = temp_registry
            self.logger.info(f"Built evidence registry with {len(current_evidence_registry)} items.")
        
        # Generate views using smart view generator with deduplicated insights
        views = self.view_generator.generate_views(
            destination_data, 
            current_evidence_registry, 
            themes_data
        )
        
        export = {
            "export_metadata": {
                "version": self.export_config.version,
                "export_timestamp": datetime.now().isoformat(),
                "destination_id": destination.id,
                "destination_revision": destination.destination_revision,
                "export_type": "comprehensive_consolidated",
                "export_mode": self.export_config.mode.value,
                "format": "reference_based_configurable",
                "deduplication_applied": {
                    "evidence_deduplication": bool(current_evidence_registry), # Use current_evidence_registry
                    "insights_deduplication": True,
                    "cross_referencing_fixed": True
                }
            },
            
            # Core destination information
            "destination": destination_data,
            
            # Data stores - single source of truth for each data type
            "data": {
                "evidence": current_evidence_registry, # Use current_evidence_registry
                "themes": themes_data,
                "insights": deduplicated_insights,  # Now deduplicated with no cross-reference issues
                "authorities": self._format_local_authorities(destination.local_authorities),
                "temporal_slices": self._format_temporal_slices(destination.temporal_slices),
                "dimensions": self._format_dimensions(destination.dimensions),
                "points_of_interest": self._format_pois(destination.pois)
            },
            
            # Relationship mappings for references (now includes deduplicated insights)
            "relationships": {
                "theme_evidence": self._build_theme_evidence_relationships(destination.themes),
                "theme_insights": insights_cross_refs.get("theme_insights", {}),  # From deduplicator
                "insight_themes": insights_cross_refs.get("insight_themes", {}),  # Reverse mapping
                "destination_insights": insights_cross_refs.get("destination_insights", {}),  # From deduplicator
                "insight_authorities": self._build_insight_authority_relationships_deduplicated(deduplicated_insights),
                "temporal_themes": self._build_temporal_theme_relationships(destination.temporal_slices)
            },
            
            # Smart generated views
            "views": views,
            
            # Analysis metadata and lineage
            "metadata": {
                "analysis_metadata": analysis_metadata or {},
                "lineage": destination.lineage,
                "processing_metadata": destination.meta,
                "data_quality": self._calculate_comprehensive_quality_metrics(destination, current_evidence_registry), # Use current_evidence_registry
                "export_configuration": self.export_config.dict(),
                "deduplication_statistics": {
                    "insights_stats": self.insights_deduplicator.get_registry().get_cross_reference_statistics()
                }
            }
        }
        
        return export
    
    def _create_minimal_export(
        self, 
        destination: Destination,
        evidence_registry: Optional[Dict[str, Dict[str, Any]]] = None
    ) -> Dict[str, Any]:
        """Create minimal export with essential data only"""
        
        destination_data = self._format_destination_data(destination)
        
        # Only include high-confidence themes
        high_conf_themes = {}
        for theme in destination.themes:
            if theme.get_confidence_level() in [ConfidenceLevel.HIGH, ConfidenceLevel.VERY_HIGH]:
                theme_data = self._format_single_theme(theme)
                high_conf_themes[theme.theme_id] = theme_data
        
        # Generate minimal views
        views = {
            "executive_summary": self.view_generator._generate_executive_summary(
                destination_data, high_conf_themes
            )
        }
        
        return {
            "export_metadata": {
                "version": self.export_config.version,
                "export_timestamp": datetime.now().isoformat(),
                "destination_id": destination.id,
                "export_type": "minimal_export",
                "export_mode": "minimal"
            },
            "destination": destination_data,
            "data": {
                "themes": high_conf_themes
            },
            "views": views
        }
    
    def _create_summary_export(
        self, 
        destination: Destination,
        evidence_registry: Optional[Dict[str, Dict[str, Any]]] = None
    ) -> Dict[str, Any]:
        """Create summary-only export"""
        
        destination_data = self._format_destination_data(destination)
        themes_data = self._format_themes_with_references(destination.themes)
        
        # Generate only executive summary
        summary_view = self.view_generator._generate_executive_summary(
            destination_data, themes_data
        )
        
        return {
            "export_metadata": {
                "version": self.export_config.version,
                "export_timestamp": datetime.now().isoformat(),
                "destination_id": destination.id,
                "export_type": "summary_only",
                "export_mode": "summary_only"
            },
            "destination": destination_data,
            "executive_summary": summary_view,
            "stats": {
                "total_themes": len(themes_data),
                "high_confidence_themes": len([t for t in themes_data.values() 
                                             if self._get_theme_confidence_safe(t) > 0.7])
            }
        }
    
    def _create_evidence_focused_export(
        self, 
        destination: Destination,
        evidence_registry: Optional[Dict[str, Dict[str, Any]]] = None
    ) -> Dict[str, Any]:
        """Create evidence-focused export"""
        
        destination_data = self._format_destination_data(destination)
        
        # Generate evidence-focused views
        evidence_views = {
            "evidence_by_source": self.view_generator._generate_evidence_by_source_views(evidence_registry or {}),
            "quality_dashboard": self.view_generator._generate_quality_dashboard(
                destination_data, evidence_registry or {}, {}
            )
        }
        
        return {
            "export_metadata": {
                "version": self.export_config.version,
                "export_timestamp": datetime.now().isoformat(),
                "destination_id": destination.id,
                "export_type": "evidence_focused",
                "export_mode": "evidence_focused"
            },
            "destination": destination_data,
            "data": {
                "evidence": evidence_registry or {}
            },
            "relationships": {
                "theme_evidence": self._build_theme_evidence_relationships(destination.themes)
            },
            "views": evidence_views
        }
    
    def _create_themes_focused_export(
        self, 
        destination: Destination,
        evidence_registry: Optional[Dict[str, Dict[str, Any]]] = None
    ) -> Dict[str, Any]:
        """Create themes-focused export"""
        
        # Deduplicate insights for themes-focused export
        self.insights_deduplicator.clear_registry()
        deduplicated_insights, insights_cross_refs = deduplicate_authentic_insights_for_export(destination)
        
        destination_data = self._format_destination_data(destination)
        themes_data = self._format_themes_with_references(destination.themes)
        
        # Generate theme-focused views
        theme_views = {
            "themes_by_category": self.view_generator._generate_category_views(themes_data),
            "seasonal_overview": self.view_generator._generate_seasonal_views(themes_data)
        }
        
        return {
            "export_metadata": {
                "version": self.export_config.version,
                "export_timestamp": datetime.now().isoformat(),
                "destination_id": destination.id,
                "export_type": "themes_focused",
                "export_mode": "themes_focused"
            },
            "destination": destination_data,
            "data": {
                "themes": themes_data,
                "insights": deduplicated_insights  # Now deduplicated
            },
            "relationships": {
                "theme_insights": insights_cross_refs.get("theme_insights", {}),
                "insight_themes": insights_cross_refs.get("insight_themes", {})
            },
            "views": theme_views
        }
    
    def _format_destination_data(self, destination: Destination) -> Dict[str, Any]:
        """Formats the main destination object for export, including all new enrichment fields."""
        return {
            "id": destination.id,
            "names": destination.names,
            "admin_levels": destination.admin_levels,
            "timezone": destination.timezone,
            "population": destination.population,
            "country_code": destination.country_code,
            "core_geo": destination.core_geo,
            # New heuristic-driven enrichment fields
            "vibe_descriptors": destination.vibe_descriptors,
            "area_km2": destination.area_km2,
            "primary_language": destination.primary_language,
            "dominant_religions": destination.dominant_religions,
            "unesco_sites": destination.unesco_sites,
            "historical_summary": destination.historical_summary,
            "annual_tourist_arrivals": destination.annual_tourist_arrivals,
            "popularity_stage": destination.popularity_stage,
            "visa_info_url": destination.visa_info_url,
            "gdp_per_capita_usd": destination.gdp_per_capita_usd,
            "hdi": destination.hdi,
            # Metadata
            "last_updated": destination.last_updated.isoformat(),
            "destination_revision": destination.destination_revision,
            "lineage": destination.lineage,
            "meta": destination.meta
        }

    def _format_single_theme(self, theme: Theme) -> Dict[str, Any]:
        """Formats a single theme object for export"""
        # Extract evidence references if they exist
        evidence_refs = []
        if hasattr(theme, 'evidence_references'):
            evidence_refs = theme.evidence_references
        elif hasattr(theme, 'evidence'):
            # Fallback: create simple references from evidence
            evidence_refs = [{"evidence_id": f"ev_{i}", "relevance_score": 1.0} 
                           for i, _ in enumerate(theme.evidence)]
        
        # Format confidence breakdown - ensure it's never None
        confidence_breakdown_dict = {}
        if theme.confidence_breakdown:
            # Proper type checking for confidence_breakdown including JSON strings
            if isinstance(theme.confidence_breakdown, dict):
                confidence_breakdown_dict = theme.confidence_breakdown
            elif hasattr(theme.confidence_breakdown, 'to_dict') and callable(getattr(theme.confidence_breakdown, 'to_dict')):
                confidence_breakdown_dict = theme.confidence_breakdown.to_dict()
            elif isinstance(theme.confidence_breakdown, str):
                # Handle JSON string case
                try:
                    import json
                    confidence_breakdown_dict = json.loads(theme.confidence_breakdown)
                except (json.JSONDecodeError, TypeError):
                    confidence_breakdown_dict = {"error": "invalid_json_confidence_breakdown", "original": theme.confidence_breakdown}
            else:
                confidence_breakdown_dict = {"error": "unexpected_confidence_breakdown_type", "type": str(type(theme.confidence_breakdown))}
        else:
            # Provide default confidence breakdown structure
            confidence_breakdown_dict = {
                "overall_confidence": 0.5,
                "evidence_quality": 0.5,
                "source_authority": 0.5,
                "temporal_freshness": 0.5,
                "cultural_relevance": 0.5
            }
        
        return {
            "theme_id": theme.theme_id,
            "name": theme.name,
            "macro_category": theme.macro_category,
            "micro_category": theme.micro_category,
            "description": theme.description,
            "fit_score": theme.fit_score,
            "confidence_level": theme.get_confidence_level().value,
            "confidence_breakdown": confidence_breakdown_dict,  # Now guaranteed to be a dict
            "evidence_references": evidence_refs,
            "tags": theme.tags,
            "created": theme.created_date.isoformat(),
            "last_validated": theme.last_validated.isoformat() if theme.last_validated else None,
            "factors": self._safe_get_dict_attribute(theme, 'factors'),
            "cultural_summary": self._safe_get_dict_attribute(theme, 'cultural_summary'),
            "sentiment_analysis": self._safe_get_dict_attribute(theme, 'sentiment_analysis'),
            "temporal_analysis": self._safe_get_dict_attribute(theme, 'temporal_analysis'),
            "seasonal_relevance": self._safe_get_dict_attribute(theme, 'seasonal_relevance'),
            "regional_uniqueness": getattr(theme, 'regional_uniqueness', 0.0),
            "insider_tips": getattr(theme, 'insider_tips', [])
        }

    def _format_themes_with_references(self, themes: List[Theme]) -> Dict[str, Dict[str, Any]]:
        """Format themes using reference IDs instead of embedding objects"""
        themes_dict = {}
        
        for theme in themes:
            themes_dict[theme.theme_id] = self._format_single_theme(theme)
        
        return themes_dict

    def _format_authentic_insights(self, insights: List[Any]) -> Dict[str, Dict[str, Any]]:
        """Format authentic insights with unique IDs (legacy method - prefer deduplicator)"""
        insights_dict = {}
        
        for i, insight in enumerate(insights):
            insight_id = f"ai_{i}"
            # Proper type checking for insights
            if isinstance(insight, dict):
                insights_dict[insight_id] = insight
            elif hasattr(insight, 'to_dict') and callable(getattr(insight, 'to_dict')):
                insights_dict[insight_id] = insight.to_dict()
            else:
                insights_dict[insight_id] = {"error": "unexpected_insight_type", "type": str(type(insight))}
        
        return insights_dict
    
    def _format_local_authorities(self, authorities: List[Any]) -> Dict[str, Dict[str, Any]]:
        """Format local authorities with safe ID generation"""
        authorities_dict = {}
        for i, auth in enumerate(authorities):
            # Generate safe ID if authority doesn't have one
            auth_id = getattr(auth, 'id', None) or f"la_{i}"
            # Proper type checking for authorities
            if isinstance(auth, dict):
                authorities_dict[str(auth_id)] = auth
            elif hasattr(auth, 'to_dict') and callable(getattr(auth, 'to_dict')):
                authorities_dict[str(auth_id)] = auth.to_dict()
            else:
                authorities_dict[str(auth_id)] = {"error": "unexpected_authority_type", "type": str(type(auth))}
        return authorities_dict
    
    def _format_temporal_slices(self, slices: List[TemporalSlice]) -> Dict[str, Dict[str, Any]]:
        """Formats temporal slices for export, ensuring SpecialEvent objects are serialized."""
        formatted_slices = {}
        for i, sl in enumerate(slices):
            # Proper type checking for temporal slice
            if isinstance(sl, dict):
                slice_dict = sl
            elif hasattr(sl, 'to_dict') and callable(getattr(sl, 'to_dict')):
                slice_dict = sl.to_dict()
            else:
                slice_dict = {"error": "unexpected_slice_type", "type": str(type(sl))}
            
            # Ensure special_events are dictionaries with proper type checking
            if 'special_events' in slice_dict or hasattr(sl, 'special_events'):
                special_events = slice_dict.get('special_events', []) if isinstance(sl, dict) else getattr(sl, 'special_events', [])
                slice_dict['special_events'] = [
                    event if isinstance(event, dict)
                    else event.to_dict() if hasattr(event, 'to_dict') and callable(getattr(event, 'to_dict'))
                    else {"error": "unexpected_event_type", "type": str(type(event))}
                    for event in special_events
                ]
            
            formatted_slices[f"slice_{i}"] = slice_dict
        return formatted_slices
    
    def _format_dimensions(self, dimensions: Dict[str, Any]) -> Dict[str, Dict[str, Any]]:
        """Formats dimension data for export"""
        dimensions_dict = {}
        
        for name, dim in dimensions.items():
            if dim.value is not None:
                dimensions_dict[name] = {
                    "value": dim.value,
                    "unit": dim.unit,
                    "confidence": dim.confidence,
                    "last_updated": dim.last_updated.isoformat(),
                    "evidence_count": len(dim.source_evidence_ids)
                }
                
        return dimensions_dict
    
    def _format_pois(self, pois: List[Any]) -> Dict[str, Dict[str, Any]]:
        """Format POIs with unique IDs"""
        pois_dict = {}
        
        for poi in pois:
            poi_id = poi.poi_id
            pois_dict[poi_id] = {
                "id": poi_id,
                "name": poi.name,
                "type": poi.poi_type,
                "location": poi.location,
                "theme_associations": poi.theme_tags,
                "accessibility": {
                    "ada_accessible": poi.ada_accessible,
                    "features": poi.ada_features
                }
            }
        
        return pois_dict
    
    def _build_theme_evidence_relationships(self, themes: List[Theme]) -> Dict[str, List[str]]:
        """Build theme -> evidence ID mappings"""
        relationships = {}
        
        for theme in themes:
            theme_id = theme.theme_id
            evidence_ids = []
            
            if hasattr(theme, 'evidence_references'):
                evidence_ids = [ref["evidence_id"] for ref in theme.evidence_references]
            elif hasattr(theme, 'evidence'):
                # Fallback: create evidence IDs from index
                evidence_ids = [f"ev_{i}" for i, _ in enumerate(theme.evidence)]
            
            relationships[theme_id] = evidence_ids
        
        return relationships
    
    def _build_theme_insight_relationships(self, themes: List[Theme]) -> Dict[str, List[str]]:
        """Build theme -> insight ID mappings (legacy method - prefer deduplicator)"""
        relationships = {}
        
        for theme in themes:
            theme_id = theme.theme_id
            insight_ids = []
            
            if hasattr(theme, 'authentic_insights'):
                insight_ids = [f"ai_{i}" for i, _ in enumerate(theme.authentic_insights)]
            
            relationships[theme_id] = insight_ids
        
        return relationships
    
    def _build_insight_authority_relationships_deduplicated(self, deduplicated_insights: Dict[str, Any]) -> Dict[str, List[str]]:
        """Build insight -> authority ID mappings for deduplicated insights"""
        relationships = {}
        
        for insight_id, insight_data in deduplicated_insights.items():
            authority_ids = []
            
            # Extract authority references from insight data
            if isinstance(insight_data, dict) and 'local_authorities' in insight_data:
                local_authorities = insight_data['local_authorities']
                if isinstance(local_authorities, list):
                    authority_ids = [f"la_{i}" for i, _ in enumerate(local_authorities)]
            
            relationships[insight_id] = authority_ids
        
        return relationships
    
    def _build_temporal_theme_relationships(self, slices: List[TemporalSlice]) -> Dict[str, List[str]]:
        """Build temporal slice -> theme relationships"""
        relationships = {}
        
        for i, slice_obj in enumerate(slices):
            slice_id = f"ts_{i}"
            theme_ids = []
            
            # Extract theme references from slice if available
            if hasattr(slice_obj, 'related_themes'):
                theme_ids = slice_obj.related_themes
            
            relationships[slice_id] = theme_ids
        
        return relationships
    
    def _calculate_comprehensive_quality_metrics(self, destination: Destination, evidence_registry: Dict[str, Dict[str, Any]]) -> Dict[str, Any]:
        """Calculate comprehensive quality metrics"""
        total_evidence = len(evidence_registry)
        
        confidence_scores = []
        for t in destination.themes:
            if t.confidence_breakdown:
                # Handle ConfidenceBreakdown objects, dictionaries, and JSON strings
                if hasattr(t.confidence_breakdown, 'overall_confidence'):
                    confidence_scores.append(t.confidence_breakdown.overall_confidence)
                elif isinstance(t.confidence_breakdown, dict):
                    confidence_scores.append(safe_get_confidence_value(t.confidence_breakdown, "overall_confidence", 0))
                elif isinstance(t.confidence_breakdown, str):
                    # Handle JSON string case
                    try:
                        import json
                        conf_dict = json.loads(t.confidence_breakdown)
                        if isinstance(conf_dict, dict): confidence_scores.append(safe_get(conf_dict, 'overall_confidence', 0)); 
                        else: confidence_scores.append(0)
                    except (json.JSONDecodeError, AttributeError):
                        confidence_scores.append(0)
                else:
                    confidence_scores.append(0)
        
        return {
            "total_themes": len(destination.themes),
            "total_evidence": total_evidence,
            "unique_evidence_ratio": 1.0,  # Already deduplicated
            "average_confidence": sum(confidence_scores) / len(confidence_scores) if confidence_scores else 0,
            "high_confidence_themes": len([
                t for t in destination.themes 
                if t.get_confidence_level() in [ConfidenceLevel.HIGH, ConfidenceLevel.VERY_HIGH]
            ]),
            "dimensions_populated": len([d for d in destination.dimensions.values() if d.value is not None]),
            "temporal_coverage": len(destination.temporal_slices),
            "poi_count": len(destination.pois),
            "data_completeness_score": self._calculate_overall_quality_score(destination),
            "export_efficiency_score": self._calculate_export_efficiency_score(destination, evidence_registry)
        }
    
    def _calculate_export_efficiency_score(self, destination: Destination, evidence_registry: Dict[str, Dict[str, Any]]) -> float:
        """Calculate export efficiency score based on deduplication and optimization"""
        scores = []
        
        # Evidence deduplication efficiency
        if evidence_registry:
            # Assume all evidence is deduplicated in registry
            scores.append(1.0)
        
        # Reference-based architecture efficiency
        reference_usage = 0
        total_relationships = 0
        for theme in destination.themes:
            if hasattr(theme, 'evidence_references'):
                reference_usage += 1
            total_relationships += 1
            
        if total_relationships > 0:
            reference_efficiency = reference_usage / total_relationships
            scores.append(reference_efficiency)
        
        return sum(scores) / len(scores) if scores else 0.0
    
    def _calculate_overall_quality_score(self, destination: Destination) -> float:
        """Calculate overall data quality score (0-1)"""
        scores = []
        
        # Theme quality
        if destination.themes:
            high_conf_ratio = len([t for t in destination.themes if t.get_confidence_level() in [ConfidenceLevel.HIGH, ConfidenceLevel.VERY_HIGH]]) / len(destination.themes)
            scores.append(high_conf_ratio)
        
        # Data completeness
        has_temporal = 1.0 if destination.temporal_slices else 0.0
        has_dimensions = 1.0 if any(d.value is not None for d in destination.dimensions.values()) else 0.0
        has_pois = 1.0 if destination.pois else 0.0
        
        completeness = (has_temporal + has_dimensions + has_pois) / 3.0
        scores.append(completeness)
        
        return sum(scores) / len(scores) if scores else 0.0
    
    def _get_theme_confidence_safe(self, theme_data: Dict[str, Any]) -> float:
        """Safely extract confidence from theme data regardless of format"""
        confidence_breakdown = safe_get(theme_data, "confidence_breakdown", {})
        
        if not confidence_breakdown:
            return 0.0
            
        if isinstance(confidence_breakdown, dict):
            return safe_get_confidence_value(confidence_breakdown, "overall_confidence", 0.0)
        elif isinstance(confidence_breakdown, str):
            # Handle JSON string case
            try:
                import json
                conf_dict = json.loads(confidence_breakdown)
                if isinstance(conf_dict, dict):
                    return safe_get(conf_dict, "overall_confidence", 0.0)
                else:
                    return 0.0
            except (json.JSONDecodeError, AttributeError):
                return 0.0
        else:
            return 0.0
    
    def _safe_get_dict_attribute(self, obj, attr):
        """Safely get a dictionary attribute from an object"""
        if hasattr(obj, attr):
            return getattr(obj, attr)
        else:
            return {}
    
    def _save_json(self, data: Dict[str, Any], filepath: Path):
        """Save data to JSON file with configurable formatting"""
        indent = 2 if self.export_config.pretty_print else None
        
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(data, f, indent=indent, ensure_ascii=False, default=str)
            
    def _create_latest_link(self, source_path: Path, link_path: Path):
        """Create 'latest' symlink for easy access"""
        # Remove existing symlink if it exists
        if link_path.is_symlink():
            link_path.unlink()
        elif link_path.exists():
            link_path.unlink()
            
        # Create new symlink
        try:
            link_path.symlink_to(source_path.name)
        except OSError:
            # Fall back to copying on systems that don't support symlinks
            import shutil
            shutil.copy2(source_path, link_path)
    
    def _archive_old_exports(self, destination_id: str, days_to_keep: int = 7):
        """Archive exports older than specified days"""
        archive_date = datetime.now().timestamp() - (days_to_keep * 24 * 60 * 60)
        
        for filepath in self.consolidated_path.glob(f"{destination_id}_*_*.json"):
            if filepath.stat().st_mtime < archive_date:
                archive_file = self.base_path / "archive" / filepath.name
                filepath.rename(archive_file)
                self.logger.info(f"Archived old export: {filepath.name}") 